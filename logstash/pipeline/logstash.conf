input {
  file {
    max_open_files => 10
    mode => "read"
    file_chunk_size => 327680
    path => ["/client_data/nodes/*/logs/cassandra/system.log", "/client_data/nodes/*/logs/cassandra/debug.log"]
    start_position => "beginning"
    file_completed_action => "log"
    file_completed_log_path => "/client_data/logstash-completed.log"
    sincedb_path => "/dev/null"
    codec => multiline {
      patterns_dir => ["/usr/share/logstash/config/patterns"]
      pattern => "%{CASS_DEFAULT}"
      negate => true
      what => "previous"
    }
  }
}

## Add your filters / logstash plugins configuration here

filter {
  fingerprint {
    source => "message"
    target => "[@metadata][fingerprint]"
    method => "MURMUR3"
  }
}

filter {
  grok {
    patterns_dir => ["/usr/share/logstash/config/patterns"]
    break_on_match => true
    match => {"message" => ["%{CASS_DEFAULT}"]}
    overwrite => ["message", "@timestamp", "level", "process", "file"]
    add_tag => ["cass_default"]
  }

  date {
    locale => "en"
    match => ["timestamp", "YYYY-MM-dd HH:mm:ss,SSS"] # 2016-08-16 04:13:05,063
    timezone => "Etc/UTC"
    target => "@timestamp"
  }

  grok {
    match => {"path" => "%{GREEDYDATA}%{IPV4:node:ip}%{GREEDYDATA}"}
  }

  grok {
    match => {"process" => "STREAM-(IN|OUT|INIT)-\/%{IPV4:stream_peer:ip}"}
  }

  ### Place additional pattern matching here ###
  if [file] == "GCInspector.java" {
    grok {
      patterns_dir => "/usr/share/logstash/config/patterns"
      match => {"message" => "G1\s?%{GCINSPECTOR_GC_TYPE:gc_type} GC in %{GCINSPECTOR_GC_DURATION_MS:gc_duration_ms:int}ms%{GREEDYDATA}"}
      tag_on_failure => ["tags", "gcinspector_grok_parse_failure"]
    }
  }
  if [file] == "HintsDispatchExecutor.java" {
    grok {
      patterns_dir => "/usr/share/logstash/config/patterns"
      match => {"message" => "%{GREEDYDATA}\/%{IP:hint_recipient:ip}: %{UUID}"}
    }
  }
  if [file] == "SolrMetricsEventListener.java" {
    grok {
      patterns_dir => "/usr/share/logstash/config/patterns"
      match => {
        "message" => ["Lucene flush took %{BASE10NUM:lucene_flush_duration_secs:float} seconds with %{BASE10NUM:lucene_flush_segments:int} segments flushed and %{BASE10NUM:lucene_flush_size_mb:float} MB flushed",
          "Lucene merge took %{BASE10NUM:lucene_merge_duration_secs:float} seconds with %{BASE10NUM:lucene_merge_size_mb:float} MB merged and %{BASE10NUM:lucene_merge_size_estimated_mb:float} MB estimated merged"]
      }
    }
  }
  if [file] == "SolrFilterCache.java" {
    grok {
      patterns_dir => "/usr/share/logstash/config/patterns"
      match => {
        "message" => ["...eviction completed in %{BASE10NUM:filter_cache_eviction_duration_ms:int} milliseconds. Filter cache %{GREEDYDATA} usage is now %{BASE10NUM:filter_cache_bytes:int} bytes across %{BASE10NUM:filter_cache_entries:int} entries."]
      }
    }
  }
  if [file] == "IndexWriter.java" {
    grok {
      patterns_dir => "/usr/share/logstash/config/patterns"
      match => {
        "message" => ["commitInternalComplete duration=%{BASE10NUM:hard_commit_duration_ms:int} ms%{GREEDYDATA}"]
      }
    }
  }
  if [file] == "Gossiper.java" {
    grok {
      patterns_dir => "/usr/share/logstash/config/patterns"
      match => {
        "message" => ["InetAddress /%{IPV4:gossip_node_down_ip} is now DOWN",
                      "InetAddress /%{IPV4:gossip_node_up_ip} is now UP"]
      }
    }
  }
  if [file] == "FailureDetector.java" {
    grok {
      patterns_dir => "/usr/share/logstash/config/patterns"
      match => {
        "message" => ["Not marking nodes down due to local pause of %{BASE10NUM:gossip_local_pause_duration_nanos:int} > 5000000000"]
      }
    }
  }
  if [file] == "ReadCommand.java" {
    grok {
      patterns_dir => "/usr/share/logstash/config/patterns"
      match => {
        "message" => ["Read %{BASE10NUM:tombstone_warn_live:int} live rows and %{BASE10NUM:tombstone_warn_tombstone:int} tombstone cells for query %{GREEDYDATA}"]
      }
    }
  }
  if [file] == "ColumnFamilyStore.java" {
    grok {
      patterns_dir => "/usr/share/logstash/config/patterns"
      match => {
        "message" => ["Enqueuing flush of %{DATA:flush_table_name}: %{DATA:flush_size_on_heap} \(%{BASE10NUM:flush_percentage_on_heap}\%\) on-heap, %{DATA:flush_size_off_heap} \(%{BASE10NUM:flush_percentage_off_heap}\%\) off-heap"]
      }
    }
    mutate {
      gsub => [
        "process", "\(([0-9]*)\)", ""
      ]
   }
  }

  if [file] == "AbstractOutboundMessageHandler.java" {
    grok {
      patterns_dir => "/usr/share/logstash/config/patterns"
      match => {
        "message" => ["%{GREEDYDATA:message_type} with id %{BASE10NUM} from /%{IPV4:message_from} to /%{IPV4:message_to} via \(/%{IPV4},/%{IPV4}:%{BASE10NUM}\) error...(.|\r|\n)*%{GREEDYDATA} failed: Connection (?<error_type>(reset|timed out))%{GREEDYDATA}"]
      }
    }
  }
  if [file] == "SyncTask.java" {
    grok {
      patterns_dir => "/usr/share/logstash/config/patterns"
      match => {
        "message" => ["%{GREEDYDATA} Endpoints %{GREEDYDATA} have %{BASE10NUM:repair_ranges_out_of_sync:int} range\(s\) out of sync for %{USERNAME:repair_table_name}%{GREEDYDATA}?"]
      }
    }
  }
  if [file] == "QueryComponent.java" {
    grok {
      patterns_dir => "/usr/share/logstash/config/patterns"
      match => {
        "message" => ["process: distrib\=false&qt\=solr_query&ShardRouter.SHARD_COORDINATOR_ID\=%{UUID:shard_coordinator_id}%{DATA}fq\=%{DATA}party_address_analyzed\:%{DATA:party_address_analyzed}\+AND\+party_name_analyzed%{GREEDYDATA}AND\+party_phone_analyzed\:%{NUMBER:party_phone_analyzed}%{GREEDYDATA}timeAllowed\=60000"]
      }
    }
  }
  if [file] == "CompactionTask.java" {
    grok {
      patterns_dir = > "/usr/share/logstash/config/patterns"
      match = > {
        "message" = > ["Compacted \(%{UUID:compaction_uuid}\) %{INT:compaction_num_sstables} sstables to \[/data_02/data/%{DATA:compaction_keyspace}/%{DATA:compaction_table_name}-.*"]
      }
    }
  }
  if [file] == "MessagingService.java" {
    grok {
      patterns_dir => "/usr/share/logstash/config/patterns"
      match => {
        "message" => ["%{DATA:dropped_message_type} messages were dropped in last %{INT:dropped_message_duration:int} ms: %{INT:dropped_message_internal_count:int} internal and %{INT:dropped_message_crossnode_count:int} cross node. Mean internal dropped latency: %{INT:dropped_message_internal_mean_latency:int} ms and Mean cross-node dropped latency: %{INT:dropped_message_crossnode_mean_latency:int} ms"]
      }
    }
  }
  ### ###

  mutate {
    convert => {
      "line_number" => "integer"
      "threadId" => "integer"
      "bytes_in" => "integer"
      "bytes_out" => "integer"
      "time_ms" => "integer"
      "bytes_onheap" => "integer"
      "bytes_offheap" => "integer"
      "commitlog_pos" => "integer"
      "eden_orig_bytes" => "integer"
      "eden_new_bytes" => "integer"
      "oldgen_orig_bytes" => "integer"
      "oldgen_new_bytes" => "integer"
      "survivor_new_bytes" => "integer"
      "survivor_orig_bytes" => "integer"
      "commitlog_segid" => "integer"
      "ops" => "integer"
      "pkeys_in" => "integer"
      "pkeys_out" => "integer"
      "percent_of_orig" => "integer"
      "cache_size" => "integer"
      "cache_used" => "integer"

      "threads_active" => "integer"
      "threads_pending" => "integer"
      "threads_blocked" => "integer"
      "threads_completed" => "integer"
      "threads_all_time_blocked" => "integer"

      "size_kb" => "float"
      "size_mb" => "float"
      "total_onheap" => "float"
      "total_offheap" => "float"
      "live_onheap" => "float"
      "live_offheap" => "float"
      "flushing_onheap" => "float"
      "flushing_offheap" => "float"
      "this_onheap" => "float"
      "this_offheap" => "float"
      "percent_onheap" => "float"
      "percent_offheap" => "float"
      "speed_mb" => "float"
    }
  }

  if [host] {
    mutate {
      remove_field => "host"
    }
  }
  # drop extra lines that weren't matched and are missing file tags
  if ![file] {
    drop { }
  }

}


output {
  if "gcinspector_grok_parse_failure" in [tags] {
    file { "path" => "/tmp/grok_failures.txt" }
  } else {
    elasticsearch {
      hosts => "elasticsearch:9200"
      index => "logstash"
      document_id => "%{[@metadata][fingerprint]}"
    }
    stdout {
      codec => dots
    }
  }
}
